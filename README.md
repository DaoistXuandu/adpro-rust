## Commit 1 Reflection notes
The handle_connection function in Rust processes an incoming TCP stream by reading HTTP request headers. It starts by creating a BufReader to efficiently handle stream data, then reads the request line by line using .lines(). The .map(|result| result.unwrap()) extracts the actual string, and .take_while(|line| !line.is_empty()) ensures only the headers are captured before collecting them into a Vec. Finally, it prints the request using println!("Request: {:#?}", http_request);, providing a structured debug output.

This function demonstrates efficient buffered reading and line-by-line parsing of an HTTP request. However, using .unwrap() can lead to panics if errors occur, so a more robust error-handling approach would be beneficial. The function is a foundational component of a basic HTTP server, helping to parse requests before processing and responding accordingly.

## Commit 2 Reflection notes
![Image - 1](/static/image1.png)

## Commit 3 Reflection notes
![Image - 2](/static/image2.png)
The refactoring process improves code clarity and organization by separating request handling and response selection. In the previous implementation, the code read the entire HTTP request into a vector and then proceeded directly to responding with a fixed file. This approach did not distinguish between different requests, always returning the same file regardless of what was requested.

The refactored version improves upon this by extracting only the first line of the request, which contains the method and requested path. By checking if the request matches "GET / HTTP/1.1", the function can determine whether to return the main page or a 404 response. This makes the server more dynamic, allowing different responses based on the request. Additionally, the refactoring simplifies the code by removing unnecessary collection, repeatable code and iteration over request lines, making it more efficient and easier to maintain.

## Commit 4 Reflection notes
When I tested the server, I noticed that accessing 127.0.0.1/sleep caused a delay of five seconds before the page loaded. During this time, if I tried to access 127.0.0.1 in another browser window, it also got stuck waiting. This happened because the server processes requests sequentially in a single thread. The thread sleep function completely blocks the server, meaning that while one request is being handled, no other requests can be processed. This made me realize that if multiple users accessed the sleep route, the entire server would slow down significantly. The issue is that the server executes everything one at a time without handling multiple requests concurrently. To improve this, I would need to implement multi-threading or asynchronous programming. Using a thread pool or an async runtime like tokio would allow the server to handle multiple requests at the same time, making it much more efficient. This experience helped me understand why blocking operations can be a problem in web servers and how concurrency can solve it.

## Commit 5 Reflection notes
The updated server implementation now incorporates a ThreadPool to handle multiple requests concurrently, improving efficiency and responsiveness. In lib.rs, the ThreadPool struct is defined with a vector of worker threads and an mpsc::Sender<Job> for message passing. The Job type is defined as a Box<dyn FnOnce() + Send + 'static>, allowing tasks to be executed dynamically. The new function initializes the thread pool with a specified number of worker threads, ensuring that at least one thread is created. A message queue is set up using Rust’s mpsc::channel(), where the sender is stored in the ThreadPool struct, and the receiver is wrapped in an Arc<Mutex<>> to allow multiple worker threads to safely access it. The workers are then created, each spawning a thread that listens for incoming jobs.

Each worker, represented by the Worker struct, consists of an ID and a thread handle. When a worker is initialized, it enters a loop where it continuously locks the receiver, retrieves a job from the queue, and executes it. The loop ensures that workers remain active and ready to process incoming requests. The execute method of ThreadPool takes a function as input, boxes it into a Job, and sends it through the message channel to be picked up by an available worker. This allows tasks to be distributed efficiently across multiple threads, preventing the main server thread from blocking due to long-running operations.

In main.rs, the TcpListener binds to 127.0.0.1:7878 and listens for incoming connections. A ThreadPool with four workers is created, meaning that up to four requests can be processed concurrently. When a connection is received, the execute method of the thread pool assigns it to an available worker. The handle_connection function reads the request and determines the appropriate response based on the requested route. If the request is for /sleep, the function introduces a five-second delay before sending the response. However, unlike the previous implementation, this delay does not block the server from handling additional requests, as other worker threads remain available to process them.

By implementing a thread pool, the server achieves greater concurrency and efficiency, addressing the issue of request blocking. Without this improvement, slow requests such as /sleep would halt the server’s ability to handle new connections. Now, with a fixed number of worker threads, the server can manage multiple clients simultaneously, distributing workloads effectively. This approach prevents excessive thread creation, which could otherwise lead to performance degradation due to high resource consumption. The implementation demonstrates an efficient way to handle concurrent connections in Rust, making the server more scalable and responsive.