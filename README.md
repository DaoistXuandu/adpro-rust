## Commit 1 Reflection notes
The handle_connection function in Rust processes an incoming TCP stream by reading HTTP request headers. It starts by creating a BufReader to efficiently handle stream data, then reads the request line by line using .lines(). The .map(|result| result.unwrap()) extracts the actual string, and .take_while(|line| !line.is_empty()) ensures only the headers are captured before collecting them into a Vec. Finally, it prints the request using println!("Request: {:#?}", http_request);, providing a structured debug output.

This function demonstrates efficient buffered reading and line-by-line parsing of an HTTP request. However, using .unwrap() can lead to panics if errors occur, so a more robust error-handling approach would be beneficial. The function is a foundational component of a basic HTTP server, helping to parse requests before processing and responding accordingly.

## Commit 2 Reflection notes
![Image - 1](/static/image1.png)

## Commit 3 Reflection notes
![Image - 2](/static/image2.png)
The refactoring process improves code clarity and organization by separating request handling and response selection. In the previous implementation, the code read the entire HTTP request into a vector and then proceeded directly to responding with a fixed file. This approach did not distinguish between different requests, always returning the same file regardless of what was requested.

The refactored version improves upon this by extracting only the first line of the request, which contains the method and requested path. By checking if the request matches "GET / HTTP/1.1", the function can determine whether to return the main page or a 404 response. This makes the server more dynamic, allowing different responses based on the request. Additionally, the refactoring simplifies the code by removing unnecessary collection, repeatable code and iteration over request lines, making it more efficient and easier to maintain.

## Commit 4 Reflection notes
When I tested the server, I noticed that accessing 127.0.0.1/sleep caused a delay of five seconds before the page loaded. During this time, if I tried to access 127.0.0.1 in another browser window, it also got stuck waiting. This happened because the server processes requests sequentially in a single thread. The thread sleep function completely blocks the server, meaning that while one request is being handled, no other requests can be processed. This made me realize that if multiple users accessed the sleep route, the entire server would slow down significantly. The issue is that the server executes everything one at a time without handling multiple requests concurrently. To improve this, I would need to implement multi-threading or asynchronous programming. Using a thread pool or an async runtime like tokio would allow the server to handle multiple requests at the same time, making it much more efficient. This experience helped me understand why blocking operations can be a problem in web servers and how concurrency can solve it.